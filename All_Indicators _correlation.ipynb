{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import yfinance as yf\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from ta import add_all_ta_features\n",
    "from ta.utils import dropna\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def analyze_technical_correlations(ticker):\n",
    "    # Step 1: Download stock data\n",
    "    print(f\"üì• Downloading data for {ticker}...\")\n",
    "    end_date = datetime.today()\n",
    "    start_date = end_date - relativedelta(years=10)\n",
    "\n",
    "    df = yf.download(ticker, start=start_date, end=end_date, interval=\"1d\", auto_adjust=True)\n",
    "    if df.empty:\n",
    "        print(f\"‚ùå No data retrieved for {ticker}.\")\n",
    "        return\n",
    "\n",
    "    df.reset_index(inplace=True)\n",
    "    df.columns = [col[0] if isinstance(col, tuple) else col for col in df.columns]\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df.set_index('Date', inplace=True)\n",
    "\n",
    "    # Step 2: Add all TA-lib indicators\n",
    "    print(\"üß† Computing all technical indicators...\")\n",
    "    df = dropna(df)\n",
    "    df = add_all_ta_features(df,\n",
    "                             open=\"Open\", high=\"High\", low=\"Low\",\n",
    "                             close=\"Close\", volume=\"Volume\",\n",
    "                             fillna=True)\n",
    "\n",
    "    df.reset_index(inplace=True)\n",
    "    \n",
    "    # Step 3: Normalize data\n",
    "    print(\"üìä Normalizing numeric features...\")\n",
    "    df['Returns'] = df['Close'].pct_change()\n",
    "    df['Lagged_Returns'] = df['Returns'].shift(1)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "\n",
    "    # Step 4: Select indicator columns (exclude target variables)\n",
    "    exclude_cols = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Returns', 'Lagged_Returns']\n",
    "    indicator_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "\n",
    "    # Step 5: Correlation Analysis\n",
    "    print(\"üîç Calculating correlations...\")\n",
    "    pearson_results = {'Price': {}, 'Returns': {}, 'Lagged_Returns': {}}\n",
    "    spearman_results = {'Price': {}, 'Returns': {}, 'Lagged_Returns': {}}\n",
    "\n",
    "    def calculate_correlations(data, x_col, y_col, lag=0):\n",
    "        x = data[x_col].shift(lag) if lag > 0 else data[x_col]\n",
    "        y = data[y_col]\n",
    "        valid_data = pd.concat([x, y], axis=1).replace([np.inf, -np.inf], np.nan).dropna()\n",
    "        if len(valid_data) < 3:\n",
    "            return np.nan, np.nan\n",
    "        try:\n",
    "            p_corr, _ = pearsonr(valid_data.iloc[:, 0], valid_data.iloc[:, 1])\n",
    "            s_corr, _ = spearmanr(valid_data.iloc[:, 0], valid_data.iloc[:, 1])\n",
    "            return p_corr, s_corr\n",
    "        except:\n",
    "            return np.nan, np.nan\n",
    "\n",
    "    for col in indicator_cols:\n",
    "        for label, y_col, lag in [('Price', 'Close', 0), ('Returns', 'Returns', 0), ('Lagged_Returns', 'Returns', 1)]:\n",
    "            p_corr, s_corr = calculate_correlations(df, col, y_col, lag)\n",
    "            pearson_results[label][col] = p_corr\n",
    "            spearman_results[label][col] = s_corr\n",
    "\n",
    "    pearson_df = pd.DataFrame(pearson_results)\n",
    "    spearman_df = pd.DataFrame(spearman_results)\n",
    "\n",
    "    # Step 6: Save correlation data\n",
    "    # pearson_df.to_csv(f\"{ticker}_pearson_correlations.csv\")\n",
    "    # spearman_df.to_csv(f\"{ticker}_spearman_correlations.csv\")\n",
    "\n",
    "    # # Step 7: Visualize with heatmaps\n",
    "    # print(\"üìà Creating heatmaps...\")\n",
    "    # plt.figure(figsize=(14, 10))\n",
    "    # sns.heatmap(pearson_df, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0)\n",
    "    # plt.title(f\"{ticker} - Pearson Correlation Heatmap\")\n",
    "    # plt.tight_layout()\n",
    "    # plt.savefig(f\"{ticker}_pearson_heatmap.png\")\n",
    "    # plt.show()\n",
    "\n",
    "    # plt.figure(figsize=(14, 10))\n",
    "    # sns.heatmap(spearman_df, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0)\n",
    "    # plt.title(f\"{ticker} - Spearman Correlation Heatmap\")\n",
    "    # plt.tight_layout()\n",
    "    # plt.savefig(f\"{ticker}_spearman_heatmap.png\")\n",
    "    # plt.show()\n",
    "\n",
    "    # Step 8: Show strong correlations\n",
    "    print(\"\\nüìå Significant Pearson Correlations (|corr| > 0.7):\")\n",
    "    print(pearson_df[(pearson_df['Price'].abs() > 0.7)])\n",
    "    # store them in a file \n",
    "    pearson_df[(pearson_df['Price'].abs() > 0.7)].to_csv(f\"Pearsons_Coeff/{ticker}_significant_pearson.csv\")\n",
    "\n",
    "    print(\"\\nüìå Significant Spearman Correlations (|corr| > 0.7):\")\n",
    "    print(spearman_df[(spearman_df['Returns'].abs() > 0.7)])\n",
    "\n",
    "\n",
    "\n",
    "    # def plot_significant_indicators(df, indicator_list):\n",
    "    #     print(\"\\nüìâ Plotting significant indicators with Close price...\")\n",
    "    #     df_plot = df.set_index('Date')\n",
    "    #     for indicator in indicator_list:\n",
    "    #         if indicator not in df_plot.columns:\n",
    "    #             continue\n",
    "    #         plt.figure(figsize=(14, 6))\n",
    "    #         plt.plot(df_plot['Close'], label='Close Price', linewidth=2)\n",
    "    #         plt.plot(df_plot[indicator], label=indicator, linestyle='--')\n",
    "    #         plt.title(f\"{ticker} - Close vs {indicator}\")\n",
    "    #         plt.xlabel(\"Date\")\n",
    "    #         plt.ylabel(\"Normalized Value\")\n",
    "    #         plt.legend()\n",
    "    #         plt.tight_layout()\n",
    "    #         # plt.savefig(f\"{ticker}_Close_vs_{indicator}.png\")\n",
    "    #         plt.show()\n",
    "\n",
    "    # significant_indicators = pearson_df.index[pearson_df['Price'].abs() > 0.7].tolist()\n",
    "    # plot_significant_indicators(df, significant_indicators)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Downloading data for PERSISTENT.NS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "/Users/utsavdhanuka/miniconda3/envs/env_pytorch/lib/python3.9/site-packages/ta/trend.py:1030: FutureWarning: Series.__setitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To set a value by position, use `ser.iloc[pos] = value`\n",
      "  self._psar[i] = high2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Computing all technical indicators...\n",
      "üìä Normalizing numeric features...\n",
      "üîç Calculating correlations...\n",
      "\n",
      "üìå Significant Pearson Correlations (|corr| > 0.7):\n",
      "                            Price   Returns  Lagged_Returns\n",
      "volume_obv               0.913873  0.032441        0.024162\n",
      "volume_vpt               0.933724  0.028945        0.017329\n",
      "volume_vwap              0.998582  0.008489        0.006217\n",
      "volatility_bbm           0.997580  0.007176        0.006108\n",
      "volatility_bbh           0.997311  0.008696        0.007447\n",
      "volatility_bbl           0.995730  0.005412        0.004553\n",
      "volatility_kcc           0.998796  0.007942        0.006429\n",
      "volatility_kch           0.998656  0.008059        0.006448\n",
      "volatility_kcl           0.998798  0.007817        0.006407\n",
      "volatility_dcl           0.996313  0.007298        0.004764\n",
      "volatility_dch           0.997593  0.010424        0.007111\n",
      "volatility_dcm           0.997770  0.008990        0.006034\n",
      "volatility_atr           0.946409  0.011932        0.007352\n",
      "trend_sma_fast           0.998621  0.007900        0.006234\n",
      "trend_sma_slow           0.996764  0.006580        0.005778\n",
      "trend_ema_fast           0.998992  0.009197        0.006115\n",
      "trend_ema_slow           0.997593  0.007413        0.005921\n",
      "trend_ichimoku_conv      0.999040  0.010537        0.006776\n",
      "trend_ichimoku_base      0.997050  0.008637        0.005948\n",
      "trend_ichimoku_a         0.998427  0.009598        0.006367\n",
      "trend_ichimoku_b         0.993884  0.007634        0.005686\n",
      "trend_visual_ichimoku_a  0.984510  0.003503        0.003481\n",
      "trend_visual_ichimoku_b  0.980464  0.004552        0.004468\n",
      "trend_psar_up            0.996618  0.004486        0.007255\n",
      "trend_psar_down          0.995202  0.002347        0.005050\n",
      "momentum_kama            0.998544  0.009015        0.005989\n",
      "others_cr                1.000000  0.026208        0.005791\n",
      "\n",
      "üìå Significant Spearman Correlations (|corr| > 0.7):\n",
      "               Price  Returns  Lagged_Returns\n",
      "others_dr   0.060214      1.0        0.013755\n",
      "others_dlr  0.060214      1.0        0.013755\n",
      "üì• Downloading data for LTIM.NS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "/Users/utsavdhanuka/miniconda3/envs/env_pytorch/lib/python3.9/site-packages/ta/trend.py:1030: FutureWarning: Series.__setitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To set a value by position, use `ser.iloc[pos] = value`\n",
      "  self._psar[i] = high2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß† Computing all technical indicators...\n",
      "üìä Normalizing numeric features...\n",
      "üîç Calculating correlations...\n",
      "\n",
      "üìå Significant Pearson Correlations (|corr| > 0.7):\n",
      "                            Price   Returns  Lagged_Returns\n",
      "volume_adi               0.804057  0.008795       -0.036834\n",
      "volume_obv               0.972377 -0.021606       -0.035083\n",
      "volume_vpt               0.973217 -0.023971       -0.035908\n",
      "volume_vwap              0.997035 -0.037614       -0.042542\n",
      "volume_nvi              -0.868729  0.022731        0.014547\n",
      "volatility_bbm           0.994679 -0.040832       -0.043124\n",
      "volatility_bbh           0.991501 -0.041910       -0.044406\n",
      "volatility_bbl           0.993506 -0.039391       -0.041437\n",
      "volatility_kcc           0.997448 -0.038726       -0.041770\n",
      "volatility_kch           0.997211 -0.038914       -0.042012\n",
      "volatility_kcl           0.997398 -0.038517       -0.041502\n",
      "volatility_dcl           0.994454 -0.036195       -0.041411\n",
      "volatility_dch           0.992637 -0.037111       -0.043896\n",
      "volatility_dcm           0.995065 -0.036745       -0.042812\n",
      "volatility_atr           0.862684 -0.037611       -0.043914\n",
      "trend_sma_fast           0.997067 -0.038596       -0.041934\n",
      "trend_sma_slow           0.992715 -0.042206       -0.044088\n",
      "trend_ema_fast           0.997851 -0.036535       -0.042350\n",
      "trend_ema_slow           0.994614 -0.040268       -0.043310\n",
      "trend_ichimoku_conv      0.997944 -0.033281       -0.041344\n",
      "trend_ichimoku_base      0.993292 -0.038281       -0.043476\n",
      "trend_ichimoku_a         0.996437 -0.035811       -0.042445\n",
      "trend_ichimoku_b         0.986882 -0.039378       -0.043345\n",
      "trend_visual_ichimoku_a  0.962429 -0.047400       -0.047301\n",
      "trend_visual_ichimoku_b  0.951645 -0.049556       -0.049667\n",
      "trend_psar_up            0.991124 -0.046597       -0.043089\n",
      "trend_psar_down          0.990059 -0.046598       -0.043392\n",
      "momentum_kama            0.997750 -0.035749       -0.041634\n",
      "others_cr                1.000000 -0.006396       -0.040746\n",
      "\n",
      "üìå Significant Spearman Correlations (|corr| > 0.7):\n",
      "              Price  Returns  Lagged_Returns\n",
      "others_dr   0.02081      1.0        0.046204\n",
      "others_dlr  0.02081      1.0        0.046204\n"
     ]
    }
   ],
   "source": [
    "# analyze_technical_correlations(\"TCS.NS\")  #Large Cap IT\n",
    "# analyze_technical_correlations(\"INFY.NS\") #large Cap IT\n",
    "# analyze_technical_correlations(\"WIPRO.NS\")  #Large Cap IT\n",
    "# analyze_technical_correlations(\"HCLTECH.NS\") # Large Cap IT\n",
    "# Mid Cap IT\n",
    "# analyze_technical_correlations(\"TECHM.NS\")  # Mid Cap IT\n",
    "analyze_technical_correlations(\"PERSISTENT.NS\")  # Mid Cap IT\n",
    "# Small Cap IT\n",
    "analyze_technical_correlations(\"LTIM.NS\")  # Small Cap IT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SENSITIVITY ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensitivity_analysis(ticker, period=1):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import yfinance as yf\n",
    "    import matplotlib.pyplot as plt\n",
    "    from ta import add_all_ta_features\n",
    "    from ta.utils import dropna\n",
    "    from datetime import datetime\n",
    "    from dateutil.relativedelta import relativedelta\n",
    "    from scipy.stats import zscore\n",
    "\n",
    "    print(f\"üì• Downloading data for {ticker}...\")\n",
    "    end_date = datetime.today()\n",
    "    start_date = end_date - relativedelta(years=10)\n",
    "\n",
    "    df = yf.download(ticker, start=start_date, end=end_date, interval=\"1d\", auto_adjust=True)\n",
    "    if df.empty:\n",
    "        print(f\"‚ùå No data retrieved for {ticker}.\")\n",
    "        return\n",
    "\n",
    "    df.reset_index(inplace=True)\n",
    "    df.columns = [col[0] if isinstance(col, tuple) else col for col in df.columns]\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df.set_index('Date', inplace=True)\n",
    "\n",
    "    # Step 2: Add technical indicators\n",
    "    print(\"üß† Computing all technical indicators...\")\n",
    "    df = dropna(df)\n",
    "    df = add_all_ta_features(df,\n",
    "                             open=\"Open\", high=\"High\", low=\"Low\",\n",
    "                             close=\"Close\", volume=\"Volume\",\n",
    "                             fillna=True)\n",
    "    df.reset_index(inplace=True)\n",
    "\n",
    "    # Z-score normalization for numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    df[numeric_cols] = df[numeric_cols].apply(zscore)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # Calculate price change\n",
    "    df['Price_Change'] = df['Close'].diff(periods=period)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # Identify indicator columns (exclude OHLCV and computed returns)\n",
    "    exclude_cols = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Price_Change']\n",
    "    indicator_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "\n",
    "    # Compute sensitivity scores\n",
    "    sensitivity_scores = {}\n",
    "\n",
    "    for col in indicator_cols:\n",
    "        if df[col].isnull().any():\n",
    "            continue\n",
    "        df[f'{col}_Change'] = df[col].diff(periods=period)\n",
    "        df_filtered = df[[f'{col}_Change', 'Price_Change']].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "        # Avoid divide-by-zero\n",
    "        df_filtered = df_filtered[df_filtered[f'{col}_Change'] != 0]\n",
    "\n",
    "        if df_filtered.empty:\n",
    "            sensitivity_scores[col] = np.nan\n",
    "            continue\n",
    "\n",
    "        # Clip extreme values to prevent distortion\n",
    "        sensitivities = (df_filtered['Price_Change'] / df_filtered[f'{col}_Change']).clip(lower=-10, upper=10)\n",
    "        sensitivity_scores[col] = sensitivities.mean()\n",
    "\n",
    "    # Construct sensitivity DataFrame\n",
    "    sensitivity_df = pd.DataFrame.from_dict(sensitivity_scores, orient='index', columns=['Average_Sensitivity'])\n",
    "    sensitivity_df.dropna(inplace=True)\n",
    "\n",
    "    # Compute Z-score of sensitivities\n",
    "    sensitivity_df['Zscore_Sensitivity'] = zscore(sensitivity_df['Average_Sensitivity'])\n",
    "\n",
    "    # Sort for visualization\n",
    "    sensitivity_df.sort_values(by='Average_Sensitivity', ascending=False, inplace=True)\n",
    "    top_raw = sensitivity_df.head(20)\n",
    "\n",
    "    sensitivity_df.sort_values(by='Zscore_Sensitivity', ascending=False, inplace=True)\n",
    "    top_zscore = sensitivity_df.head(20)\n",
    "\n",
    "    # --- Plot Raw + Z-score Side-by-Side ---\n",
    "    # fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "    # Plot 1: Raw Sensitivity Scores\n",
    "    # top_raw['Average_Sensitivity'].plot(kind='bar', ax=axes[0])\n",
    "    # axes[0].set_title(f\"{ticker} - Top 20 Raw Sensitivity Scores\")\n",
    "    # axes[0].set_ylabel(\"Average ŒîPrice / ŒîIndicator\")\n",
    "    # axes[0].set_xlabel(\"Technical Indicator\")\n",
    "    # axes[0].tick_params(axis='x', rotation=90)\n",
    "    # axes[0].grid(True)\n",
    "\n",
    "    # Plot 2: Z-score Normalized Sensitivity\n",
    "    # top_zscore['Zscore_Sensitivity'].plot(kind='bar', ax=axes[1], color='orange')\n",
    "    # axes[1].set_title(f\"{ticker} - Top 20 Normalized (Z-score) Sensitivity\")\n",
    "    # axes[1].set_ylabel(\"Z-score of Sensitivity\")\n",
    "    # axes[1].set_xlabel(\"Technical Indicator\")\n",
    "    # axes[1].tick_params(axis='x', rotation=90)\n",
    "    # axes[1].grid(True)\n",
    "\n",
    "    # plt.tight_layout()\n",
    "    # plt.savefig(f\"{ticker}_sensitivity_comparison.png\")\n",
    "    # plt.show()\n",
    "\n",
    "    # üìã Print top 20 indicators\n",
    "    print(\"\\nüìå Top 20 Indicators by Raw Average Sensitivity:\")\n",
    "    print(top_raw[['Average_Sensitivity']])\n",
    "\n",
    "    print(\"\\nüìå Top 20 Indicators by Z-score Normalized Sensitivity:\")\n",
    "    print(top_zscore[['Zscore_Sensitivity']])\n",
    "    # store top 20 Z-score indicators in a file\n",
    "    top_zscore[['Zscore_Sensitivity']].to_csv(f\"Sensitivity_Zscores/{ticker}_top_sensitivity_indicators.csv\")\n",
    "\n",
    "    # Optional: Save to CSV\n",
    "    # sensitivity_df.to_csv(f\"{ticker}_sensitivity_scores.csv\")\n",
    "\n",
    "    return sensitivity_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity_analysis(\"TCS.NS\", period=5)\n",
    "sensitivity_analysis(\"INFY.NS\", period=5)\n",
    "sensitivity_analysis(\"WIPRO.NS\", period=5)\n",
    "sensitivity_analysis(\"HCLTECH.NS\", period=5)\n",
    "sensitivity_analysis(\"TECHM.NS\", period=5)\n",
    "sensitivity_analysis(\"PERSISTENT.NS\", period=5)\n",
    "sensitivity_analysis(\"LTIM.NS\", period=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
