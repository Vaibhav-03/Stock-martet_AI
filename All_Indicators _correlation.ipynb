{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import yfinance as yf\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from ta import add_all_ta_features\n",
    "from ta.utils import dropna\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def analyze_technical_correlations(ticker):\n",
    "    # Step 1: Download stock data\n",
    "    print(f\"📥 Downloading data for {ticker}...\")\n",
    "    end_date = datetime.today()\n",
    "    start_date = end_date - relativedelta(years=10)\n",
    "\n",
    "    df = yf.download(ticker, start=start_date, end=end_date, interval=\"1d\", auto_adjust=True)\n",
    "    if df.empty:\n",
    "        print(f\"❌ No data retrieved for {ticker}.\")\n",
    "        return\n",
    "\n",
    "    df.reset_index(inplace=True)\n",
    "    df.columns = [col[0] if isinstance(col, tuple) else col for col in df.columns]\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df.set_index('Date', inplace=True)\n",
    "\n",
    "    # Step 2: Add all TA-lib indicators\n",
    "    print(\"🧠 Computing all technical indicators...\")\n",
    "    df = dropna(df)\n",
    "    df = add_all_ta_features(df,\n",
    "                             open=\"Open\", high=\"High\", low=\"Low\",\n",
    "                             close=\"Close\", volume=\"Volume\",\n",
    "                             fillna=True)\n",
    "\n",
    "    df.reset_index(inplace=True)\n",
    "    \n",
    "    # Step 3: Normalize data\n",
    "    print(\"📊 Normalizing numeric features...\")\n",
    "    df['Returns'] = df['Close'].pct_change()\n",
    "    df['Lagged_Returns'] = df['Returns'].shift(1)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "\n",
    "    # Step 4: Select indicator columns (exclude target variables)\n",
    "    exclude_cols = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Returns', 'Lagged_Returns']\n",
    "    indicator_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "\n",
    "    # Step 5: Correlation Analysis\n",
    "    print(\"🔍 Calculating correlations...\")\n",
    "    pearson_results = {'Price': {}, 'Returns': {}, 'Lagged_Returns': {}}\n",
    "    spearman_results = {'Price': {}, 'Returns': {}, 'Lagged_Returns': {}}\n",
    "\n",
    "    def calculate_correlations(data, x_col, y_col, lag=0):\n",
    "        x = data[x_col].shift(lag) if lag > 0 else data[x_col]\n",
    "        y = data[y_col]\n",
    "        valid_data = pd.concat([x, y], axis=1).replace([np.inf, -np.inf], np.nan).dropna()\n",
    "        if len(valid_data) < 3:\n",
    "            return np.nan, np.nan\n",
    "        try:\n",
    "            p_corr, _ = pearsonr(valid_data.iloc[:, 0], valid_data.iloc[:, 1])\n",
    "            s_corr, _ = spearmanr(valid_data.iloc[:, 0], valid_data.iloc[:, 1])\n",
    "            return p_corr, s_corr\n",
    "        except:\n",
    "            return np.nan, np.nan\n",
    "\n",
    "    for col in indicator_cols:\n",
    "        for label, y_col, lag in [('Price', 'Close', 0), ('Returns', 'Returns', 0), ('Lagged_Returns', 'Returns', 1)]:\n",
    "            p_corr, s_corr = calculate_correlations(df, col, y_col, lag)\n",
    "            pearson_results[label][col] = p_corr\n",
    "            spearman_results[label][col] = s_corr\n",
    "\n",
    "    pearson_df = pd.DataFrame(pearson_results)\n",
    "    spearman_df = pd.DataFrame(spearman_results)\n",
    "\n",
    "    # Step 6: Save correlation data\n",
    "    # pearson_df.to_csv(f\"{ticker}_pearson_correlations.csv\")\n",
    "    # spearman_df.to_csv(f\"{ticker}_spearman_correlations.csv\")\n",
    "\n",
    "    # # Step 7: Visualize with heatmaps\n",
    "    # print(\"📈 Creating heatmaps...\")\n",
    "    # plt.figure(figsize=(14, 10))\n",
    "    # sns.heatmap(pearson_df, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0)\n",
    "    # plt.title(f\"{ticker} - Pearson Correlation Heatmap\")\n",
    "    # plt.tight_layout()\n",
    "    # plt.savefig(f\"{ticker}_pearson_heatmap.png\")\n",
    "    # plt.show()\n",
    "\n",
    "    # plt.figure(figsize=(14, 10))\n",
    "    # sns.heatmap(spearman_df, annot=True, cmap='coolwarm', vmin=-1, vmax=1, center=0)\n",
    "    # plt.title(f\"{ticker} - Spearman Correlation Heatmap\")\n",
    "    # plt.tight_layout()\n",
    "    # plt.savefig(f\"{ticker}_spearman_heatmap.png\")\n",
    "    # plt.show()\n",
    "\n",
    "    # Step 8: Show strong correlations\n",
    "    print(\"\\n📌 Significant Pearson Correlations (|corr| > 0.7):\")\n",
    "    print(pearson_df[(pearson_df['Price'].abs() > 0.7)])\n",
    "    # store them in a file \n",
    "    pearson_df[(pearson_df['Price'].abs() > 0.7)].to_csv(f\"Pearsons_Coeff/{ticker}_significant_pearson.csv\")\n",
    "\n",
    "    print(\"\\n📌 Significant Spearman Correlations (|corr| > 0.7):\")\n",
    "    print(spearman_df[(spearman_df['Returns'].abs() > 0.7)])\n",
    "\n",
    "\n",
    "\n",
    "    # def plot_significant_indicators(df, indicator_list):\n",
    "    #     print(\"\\n📉 Plotting significant indicators with Close price...\")\n",
    "    #     df_plot = df.set_index('Date')\n",
    "    #     for indicator in indicator_list:\n",
    "    #         if indicator not in df_plot.columns:\n",
    "    #             continue\n",
    "    #         plt.figure(figsize=(14, 6))\n",
    "    #         plt.plot(df_plot['Close'], label='Close Price', linewidth=2)\n",
    "    #         plt.plot(df_plot[indicator], label=indicator, linestyle='--')\n",
    "    #         plt.title(f\"{ticker} - Close vs {indicator}\")\n",
    "    #         plt.xlabel(\"Date\")\n",
    "    #         plt.ylabel(\"Normalized Value\")\n",
    "    #         plt.legend()\n",
    "    #         plt.tight_layout()\n",
    "    #         # plt.savefig(f\"{ticker}_Close_vs_{indicator}.png\")\n",
    "    #         plt.show()\n",
    "\n",
    "    # significant_indicators = pearson_df.index[pearson_df['Price'].abs() > 0.7].tolist()\n",
    "    # plot_significant_indicators(df, significant_indicators)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Downloading data for PERSISTENT.NS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "/Users/utsavdhanuka/miniconda3/envs/env_pytorch/lib/python3.9/site-packages/ta/trend.py:1030: FutureWarning: Series.__setitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To set a value by position, use `ser.iloc[pos] = value`\n",
      "  self._psar[i] = high2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Computing all technical indicators...\n",
      "📊 Normalizing numeric features...\n",
      "🔍 Calculating correlations...\n",
      "\n",
      "📌 Significant Pearson Correlations (|corr| > 0.7):\n",
      "                            Price   Returns  Lagged_Returns\n",
      "volume_obv               0.913873  0.032441        0.024162\n",
      "volume_vpt               0.933724  0.028945        0.017329\n",
      "volume_vwap              0.998582  0.008489        0.006217\n",
      "volatility_bbm           0.997580  0.007176        0.006108\n",
      "volatility_bbh           0.997311  0.008696        0.007447\n",
      "volatility_bbl           0.995730  0.005412        0.004553\n",
      "volatility_kcc           0.998796  0.007942        0.006429\n",
      "volatility_kch           0.998656  0.008059        0.006448\n",
      "volatility_kcl           0.998798  0.007817        0.006407\n",
      "volatility_dcl           0.996313  0.007298        0.004764\n",
      "volatility_dch           0.997593  0.010424        0.007111\n",
      "volatility_dcm           0.997770  0.008990        0.006034\n",
      "volatility_atr           0.946409  0.011932        0.007352\n",
      "trend_sma_fast           0.998621  0.007900        0.006234\n",
      "trend_sma_slow           0.996764  0.006580        0.005778\n",
      "trend_ema_fast           0.998992  0.009197        0.006115\n",
      "trend_ema_slow           0.997593  0.007413        0.005921\n",
      "trend_ichimoku_conv      0.999040  0.010537        0.006776\n",
      "trend_ichimoku_base      0.997050  0.008637        0.005948\n",
      "trend_ichimoku_a         0.998427  0.009598        0.006367\n",
      "trend_ichimoku_b         0.993884  0.007634        0.005686\n",
      "trend_visual_ichimoku_a  0.984510  0.003503        0.003481\n",
      "trend_visual_ichimoku_b  0.980464  0.004552        0.004468\n",
      "trend_psar_up            0.996618  0.004486        0.007255\n",
      "trend_psar_down          0.995202  0.002347        0.005050\n",
      "momentum_kama            0.998544  0.009015        0.005989\n",
      "others_cr                1.000000  0.026208        0.005791\n",
      "\n",
      "📌 Significant Spearman Correlations (|corr| > 0.7):\n",
      "               Price  Returns  Lagged_Returns\n",
      "others_dr   0.060214      1.0        0.013755\n",
      "others_dlr  0.060214      1.0        0.013755\n",
      "📥 Downloading data for LTIM.NS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "/Users/utsavdhanuka/miniconda3/envs/env_pytorch/lib/python3.9/site-packages/ta/trend.py:1030: FutureWarning: Series.__setitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To set a value by position, use `ser.iloc[pos] = value`\n",
      "  self._psar[i] = high2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Computing all technical indicators...\n",
      "📊 Normalizing numeric features...\n",
      "🔍 Calculating correlations...\n",
      "\n",
      "📌 Significant Pearson Correlations (|corr| > 0.7):\n",
      "                            Price   Returns  Lagged_Returns\n",
      "volume_adi               0.804057  0.008795       -0.036834\n",
      "volume_obv               0.972377 -0.021606       -0.035083\n",
      "volume_vpt               0.973217 -0.023971       -0.035908\n",
      "volume_vwap              0.997035 -0.037614       -0.042542\n",
      "volume_nvi              -0.868729  0.022731        0.014547\n",
      "volatility_bbm           0.994679 -0.040832       -0.043124\n",
      "volatility_bbh           0.991501 -0.041910       -0.044406\n",
      "volatility_bbl           0.993506 -0.039391       -0.041437\n",
      "volatility_kcc           0.997448 -0.038726       -0.041770\n",
      "volatility_kch           0.997211 -0.038914       -0.042012\n",
      "volatility_kcl           0.997398 -0.038517       -0.041502\n",
      "volatility_dcl           0.994454 -0.036195       -0.041411\n",
      "volatility_dch           0.992637 -0.037111       -0.043896\n",
      "volatility_dcm           0.995065 -0.036745       -0.042812\n",
      "volatility_atr           0.862684 -0.037611       -0.043914\n",
      "trend_sma_fast           0.997067 -0.038596       -0.041934\n",
      "trend_sma_slow           0.992715 -0.042206       -0.044088\n",
      "trend_ema_fast           0.997851 -0.036535       -0.042350\n",
      "trend_ema_slow           0.994614 -0.040268       -0.043310\n",
      "trend_ichimoku_conv      0.997944 -0.033281       -0.041344\n",
      "trend_ichimoku_base      0.993292 -0.038281       -0.043476\n",
      "trend_ichimoku_a         0.996437 -0.035811       -0.042445\n",
      "trend_ichimoku_b         0.986882 -0.039378       -0.043345\n",
      "trend_visual_ichimoku_a  0.962429 -0.047400       -0.047301\n",
      "trend_visual_ichimoku_b  0.951645 -0.049556       -0.049667\n",
      "trend_psar_up            0.991124 -0.046597       -0.043089\n",
      "trend_psar_down          0.990059 -0.046598       -0.043392\n",
      "momentum_kama            0.997750 -0.035749       -0.041634\n",
      "others_cr                1.000000 -0.006396       -0.040746\n",
      "\n",
      "📌 Significant Spearman Correlations (|corr| > 0.7):\n",
      "              Price  Returns  Lagged_Returns\n",
      "others_dr   0.02081      1.0        0.046204\n",
      "others_dlr  0.02081      1.0        0.046204\n"
     ]
    }
   ],
   "source": [
    "# analyze_technical_correlations(\"TCS.NS\")  #Large Cap IT\n",
    "# analyze_technical_correlations(\"INFY.NS\") #large Cap IT\n",
    "# analyze_technical_correlations(\"WIPRO.NS\")  #Large Cap IT\n",
    "# analyze_technical_correlations(\"HCLTECH.NS\") # Large Cap IT\n",
    "# Mid Cap IT\n",
    "# analyze_technical_correlations(\"TECHM.NS\")  # Mid Cap IT\n",
    "analyze_technical_correlations(\"PERSISTENT.NS\")  # Mid Cap IT\n",
    "# Small Cap IT\n",
    "analyze_technical_correlations(\"LTIM.NS\")  # Small Cap IT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SENSITIVITY ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sensitivity_analysis(ticker, period=1):\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import yfinance as yf\n",
    "    import matplotlib.pyplot as plt\n",
    "    from ta import add_all_ta_features\n",
    "    from ta.utils import dropna\n",
    "    from datetime import datetime\n",
    "    from dateutil.relativedelta import relativedelta\n",
    "    from scipy.stats import zscore\n",
    "\n",
    "    print(f\"📥 Downloading data for {ticker}...\")\n",
    "    end_date = datetime.today()\n",
    "    start_date = end_date - relativedelta(years=10)\n",
    "\n",
    "    df = yf.download(ticker, start=start_date, end=end_date, interval=\"1d\", auto_adjust=True)\n",
    "    if df.empty:\n",
    "        print(f\"❌ No data retrieved for {ticker}.\")\n",
    "        return\n",
    "\n",
    "    df.reset_index(inplace=True)\n",
    "    df.columns = [col[0] if isinstance(col, tuple) else col for col in df.columns]\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    df.set_index('Date', inplace=True)\n",
    "\n",
    "    # Step 2: Add technical indicators\n",
    "    print(\"🧠 Computing all technical indicators...\")\n",
    "    df = dropna(df)\n",
    "    df = add_all_ta_features(df,\n",
    "                             open=\"Open\", high=\"High\", low=\"Low\",\n",
    "                             close=\"Close\", volume=\"Volume\",\n",
    "                             fillna=True)\n",
    "    df.reset_index(inplace=True)\n",
    "\n",
    "    # Z-score normalization for numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "    df[numeric_cols] = df[numeric_cols].apply(zscore)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # Calculate price change\n",
    "    df['Price_Change'] = df['Close'].diff(periods=period)\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # Identify indicator columns (exclude OHLCV and computed returns)\n",
    "    exclude_cols = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume', 'Price_Change']\n",
    "    indicator_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "\n",
    "    # Compute sensitivity scores\n",
    "    sensitivity_scores = {}\n",
    "\n",
    "    for col in indicator_cols:\n",
    "        if df[col].isnull().any():\n",
    "            continue\n",
    "        df[f'{col}_Change'] = df[col].diff(periods=period)\n",
    "        df_filtered = df[[f'{col}_Change', 'Price_Change']].replace([np.inf, -np.inf], np.nan).dropna()\n",
    "\n",
    "        # Avoid divide-by-zero\n",
    "        df_filtered = df_filtered[df_filtered[f'{col}_Change'] != 0]\n",
    "\n",
    "        if df_filtered.empty:\n",
    "            sensitivity_scores[col] = np.nan\n",
    "            continue\n",
    "\n",
    "        # Clip extreme values to prevent distortion\n",
    "        sensitivities = (df_filtered['Price_Change'] / df_filtered[f'{col}_Change']).clip(lower=-10, upper=10)\n",
    "        sensitivity_scores[col] = sensitivities.mean()\n",
    "\n",
    "    # Construct sensitivity DataFrame\n",
    "    sensitivity_df = pd.DataFrame.from_dict(sensitivity_scores, orient='index', columns=['Average_Sensitivity'])\n",
    "    sensitivity_df.dropna(inplace=True)\n",
    "\n",
    "    # Compute Z-score of sensitivities\n",
    "    sensitivity_df['Zscore_Sensitivity'] = zscore(sensitivity_df['Average_Sensitivity'])\n",
    "\n",
    "    # Sort for visualization\n",
    "    sensitivity_df.sort_values(by='Average_Sensitivity', ascending=False, inplace=True)\n",
    "    top_raw = sensitivity_df.head(20)\n",
    "\n",
    "    sensitivity_df.sort_values(by='Zscore_Sensitivity', ascending=False, inplace=True)\n",
    "    top_zscore = sensitivity_df.head(20)\n",
    "\n",
    "    # --- Plot Raw + Z-score Side-by-Side ---\n",
    "    # fig, axes = plt.subplots(1, 2, figsize=(18, 6))\n",
    "\n",
    "    # Plot 1: Raw Sensitivity Scores\n",
    "    # top_raw['Average_Sensitivity'].plot(kind='bar', ax=axes[0])\n",
    "    # axes[0].set_title(f\"{ticker} - Top 20 Raw Sensitivity Scores\")\n",
    "    # axes[0].set_ylabel(\"Average ΔPrice / ΔIndicator\")\n",
    "    # axes[0].set_xlabel(\"Technical Indicator\")\n",
    "    # axes[0].tick_params(axis='x', rotation=90)\n",
    "    # axes[0].grid(True)\n",
    "\n",
    "    # Plot 2: Z-score Normalized Sensitivity\n",
    "    # top_zscore['Zscore_Sensitivity'].plot(kind='bar', ax=axes[1], color='orange')\n",
    "    # axes[1].set_title(f\"{ticker} - Top 20 Normalized (Z-score) Sensitivity\")\n",
    "    # axes[1].set_ylabel(\"Z-score of Sensitivity\")\n",
    "    # axes[1].set_xlabel(\"Technical Indicator\")\n",
    "    # axes[1].tick_params(axis='x', rotation=90)\n",
    "    # axes[1].grid(True)\n",
    "\n",
    "    # plt.tight_layout()\n",
    "    # plt.savefig(f\"{ticker}_sensitivity_comparison.png\")\n",
    "    # plt.show()\n",
    "\n",
    "    # 📋 Print top 20 indicators\n",
    "    print(\"\\n📌 Top 20 Indicators by Raw Average Sensitivity:\")\n",
    "    print(top_raw[['Average_Sensitivity']])\n",
    "\n",
    "    print(\"\\n📌 Top 20 Indicators by Z-score Normalized Sensitivity:\")\n",
    "    print(top_zscore[['Zscore_Sensitivity']])\n",
    "    # store top 20 Z-score indicators in a file\n",
    "    top_zscore[['Zscore_Sensitivity']].to_csv(f\"Sensitivity_Zscores/{ticker}_top_sensitivity_indicators.csv\")\n",
    "\n",
    "    # Optional: Save to CSV\n",
    "    # sensitivity_df.to_csv(f\"{ticker}_sensitivity_scores.csv\")\n",
    "\n",
    "    return sensitivity_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensitivity_analysis(\"TCS.NS\", period=5)\n",
    "sensitivity_analysis(\"INFY.NS\", period=5)\n",
    "sensitivity_analysis(\"WIPRO.NS\", period=5)\n",
    "sensitivity_analysis(\"HCLTECH.NS\", period=5)\n",
    "sensitivity_analysis(\"TECHM.NS\", period=5)\n",
    "sensitivity_analysis(\"PERSISTENT.NS\", period=5)\n",
    "sensitivity_analysis(\"LTIM.NS\", period=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
